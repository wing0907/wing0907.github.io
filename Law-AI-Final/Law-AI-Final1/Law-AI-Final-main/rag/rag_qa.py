# rag/rag_qa.py
# -*- coding: utf-8 -*-
import os, json, argparse, faiss, re, unicodedata
from pathlib import Path
from sentence_transformers import SentenceTransformer
import numpy as np
from textwrap import shorten
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ---------- ê³µí†µ: ë©”íƒ€/ì„ë² ë”© ---------------------------------------------------
def load_meta(meta_path: Path):
    rows=[]
    with open(meta_path, "r", encoding="utf-8") as f:
        for line in f:
            s=line.strip()
            if s: rows.append(json.loads(s))
    return rows

def embed_query(q: str, model_name: str, device: str = "cpu"):
    model = SentenceTransformer(model_name, device=device)
    vec = model.encode([q], convert_to_numpy=True, normalize_embeddings=True)
    return model, vec

# ---------- ë²•ë ¹/íŒë¡€ ìë™ ê°ì§€ -------------------------------------------------
def is_case_row(row: dict) -> bool:
    """íŒë¡€ ì¸ë±ìŠ¤ì¸ì§€ ê°ì§€: PrecService ê¸°ë°˜ í•„ë“œ/section/íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ íŒë¡€"""
    keys = set(row.keys())
    if "section" in keys and ("íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸" in keys or "ì‚¬ê±´ëª…" in keys or "ì‚¬ê±´ë²ˆí˜¸" in keys):
        return True
    # ë²•ë ¹ ì „ìš© í‚¤ê°€ ì•„ì˜ˆ ì—†ê³  ì‚¬ê±´ ê´€ë ¨ í•„ë“œê°€ ìˆìœ¼ë©´ ì¼€ì´ìŠ¤ë¡œ ë³¸ë‹¤
    if {"ì‚¬ê±´ëª…","ì‚¬ê±´ë²ˆí˜¸","ì„ ê³ ì¼ì","ë²•ì›ëª…"} & keys and not ({"article_no","unit"} & keys):
        return True
    return False

def detect_corpus_kind(rows: list) -> str:
    if not rows: return "unknown"
    return "case" if is_case_row(rows[0]) else "law"

# ---------- ë©€í‹° ì¸ë±ìŠ¤ ë¡œë”© ----------------------------------------------------
def load_bundles(index_root: Path):
    """
    index_root/<sub>/faiss.index + meta.jsonl ì„¸íŠ¸ë¥¼ ì°¾ì•„ ë¡œë“œ.
    ë°˜í™˜: [(ì½”í¼ìŠ¤ëª…, kind('law'|'case'), faiss_index, rows, display_name)]
    """
    bundles=[]
    for sub in sorted([p for p in index_root.iterdir() if p.is_dir()]):
        idx_f = sub / "faiss.index"
        meta_f = sub / "meta.jsonl"
        if idx_f.exists() and meta_f.exists():
            index = faiss.read_index(str(idx_f))
            rows = load_meta(meta_f)
            kind = detect_corpus_kind(rows)
            # ëŒ€í‘œ í‘œì‹œ ì´ë¦„
            display = rows[0].get("law", sub.name) if (kind=="law" and rows) else \
                      rows[0].get("ë²•ì›ëª…", sub.name) if rows else sub.name
            bundles.append((sub.name, kind, index, rows, display))
    if not bundles:
        raise FileNotFoundError(f"No FAISS+meta under {index_root}")
    return bundles

def retrieve_multi(bundles, qvec: np.ndarray, topk_each: int = 8):
    """
    ê° ì¸ë±ìŠ¤ì—ì„œ topk_each ê²€ìƒ‰ â†’ í•©ì³ ì ìˆ˜ ì •ë ¬.
    ë°˜í™˜: [{"score": float, "row": dict, "kind": "law"|"case"} ...]
    """
    all_hits=[]
    qv = qvec.astype(np.float32)
    for corpus, kind, index, rows, display in bundles:
        D, I = index.search(qv, topk_each)
        for idx, sc in zip(I[0], D[0]):
            if idx < 0 or idx >= len(rows): continue
            r = dict(rows[idx])
            r["_score"] = float(sc)
            if kind == "law":
                r["law"] = r.get("law", display)
            else:
                # íŒë¡€ì—ì„  í‘œì‹œ ì´ë¦„ ë³´ê°•(ì—†ì„ ê²½ìš°)
                r["ë²•ì›ëª…"] = r.get("ë²•ì›ëª…", display)
            all_hits.append({"score": float(sc), "row": r, "kind": kind})
    all_hits.sort(key=lambda x: x["score"], reverse=True)
    return all_hits

# ---------- ê³µí†µ ìœ í‹¸: ë³¸ë¬¸ í‚¤ í†µì¼ ---------------------------------------------
def _nfkc(s: str) -> str:
    return unicodedata.normalize("NFKC", s) if s else ""

def row_text(row: dict) -> str:
    """ë²•ë ¹ì€ 'text', íŒë¡€ëŠ” 'ì „ë¬¸'ì´ ë³¸ë¬¸ í‚¤ â†’ í†µì¼í•´ì„œ ë°˜í™˜"""
    return (row.get("text") or row.get("ì „ë¬¸") or "").strip()

# ---------- ì›ë¬¸ì(â‘ â‘¡â€¦â‘³) ì •ê·œí™” ìœ í‹¸ -----------------------------------------
_CIRCLED_TO_DIGIT = {
    "â‘ ":"1","â‘¡":"2","â‘¢":"3","â‘£":"4","â‘¤":"5",
    "â‘¥":"6","â‘¦":"7","â‘§":"8","â‘¨":"9","â‘©":"10",
    "â‘ª":"11","â‘«":"12","â‘¬":"13","â‘­":"14","â‘®":"15",
    "â‘¯":"16","â‘°":"17","â‘±":"18","â‘²":"19","â‘³":"20",
}
_DIGIT_TO_CIRCLED = {v:k for k,v in _CIRCLED_TO_DIGIT.items()}

def _norm_subnum(s: str) -> str:
    """í•­/í˜¸/ëª© ë²ˆí˜¸ë¥¼ NFKC + ì›ë¬¸ìâ†’ìˆ«ì ë³€í™˜"""
    s = _nfkc(s or "")
    return _CIRCLED_TO_DIGIT.get(s, s)

# ---------- í‘œê¸°/ìŠ¤ë‹ˆí« ìœ í‹¸(ë²•ë ¹) ---------------------------------------------
def extract_subnum(row) -> str:
    if row.get("unit") in {"í•­","í˜¸","ëª©"} and "::" in row.get("id",""):
        sub = row["id"].split("::", 1)[1]
        return _norm_subnum(sub)  # ì •ê·œí™” ì ìš©
    return ""

def format_ref_law(row) -> str:
    art = row.get("article_no","")
    unit= row.get("unit","")
    parts=[]
    if art: parts.append(f"ì œ{art}ì¡°")
    sub = extract_subnum(row)
    if unit == "í•­" and sub: parts.append(f"ì œ{sub}í•­")
    elif unit == "í˜¸" and sub: parts.append(f"{sub}í˜¸")
    elif unit == "ëª©" and sub: parts.append(f"{sub}ëª©")
    if not parts:
        t = row.get("title","")
        return t if t else row.get("path","")
    return " ".join(parts)

_LEADING_COUNTER = re.compile(r"^[\dâ‘ -â‘³\.\)\-\s]+")

def clean_leading_counter_law(row, text: str) -> str:
    if not text: return text
    unit = row.get("unit","")
    if unit not in {"í•­","í˜¸","ëª©"}: return text

    raw = row.get("id","").split("::",1)[1] if "::" in row.get("id","") else ""
    sub_num = _norm_subnum(raw)               # "1"
    sub_circ = _DIGIT_TO_CIRCLED.get(sub_num) # "â‘ " or None

    t = text
    # ìˆ«ì ì‹œì‘ ì œê±°
    if sub_num and t.startswith(sub_num):
        t = t[len(sub_num):]
        t = re.sub(r"^[\.\)\s\-]+", "", t)
        return t
    # ì›ë¬¸ì ì‹œì‘ ì œê±°
    if sub_circ and t.startswith(sub_circ):
        t = t[len(sub_circ):]
        t = re.sub(r"^[\.\)\s\-]+", "", t)
        return t
    return t

def format_hit_law(row, score, snippet_chars=160):
    law  = row.get("law","ë²•ë ¹")
    ref  = format_ref_law(row)
    path = row.get("path","")
    txt  = clean_leading_counter_law(row, row_text(row))
    snip = shorten(txt.replace("\n"," "), width=snippet_chars, placeholder="â€¦")
    return f"[{score:.3f}] {law} {ref} ({row.get('unit','')}) | path: {path} | \"{snip}\""

# ---------- í‘œê¸°/ìŠ¤ë‹ˆí« ìœ í‹¸(íŒë¡€) ---------------------------------------------
def tidy_date(d: str) -> str:
    # "20001222" â†’ "2000-12-22"
    if not d: return ""
    if len(d)==8 and d.isdigit():
        return f"{d[:4]}-{d[4:6]}-{d[6:8]}"
    return d

def format_ref_case(row) -> str:
    # [ëŒ€ë²•ì› 2000-12-22, 2000ë‹¤56259, ê¸‰ì—¬ë“±] ì²˜ëŸ¼ ê°„ê²°í•˜ê²Œ
    court = row.get("ë²•ì›ëª…","ë²•ì›")
    date  = tidy_date(row.get("ì„ ê³ ì¼ì",""))
    num   = row.get("ì‚¬ê±´ë²ˆí˜¸","")
    name  = row.get("ì‚¬ê±´ëª…","")
    base  = f"{court} {date}, {num}"
    return f"{base}, {name}" if name else base

def format_hit_case(row, score, snippet_chars=160):
    ref  = format_ref_case(row)
    sec  = row.get("section","")
    txt  = row_text(row)
    snip = shorten(txt.replace("\n"," "), width=snippet_chars, placeholder="â€¦")
    return f"[{score:.3f}] {ref} [{sec}] \"{snip}\""

# ---------- LLM ë¡œë”© -----------------------------------------------------------
def load_llm_local_first(model_path: str):
    mp = Path(model_path)
    if not mp.exists():
        raise FileNotFoundError(f"ëª¨ë¸ í´ë”ê°€ ì—†ìŒ: {mp}")
    print(f"ğŸ”— ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {mp}")
    tok = AutoTokenizer.from_pretrained(str(mp), local_files_only=True)
    # pad í† í° ì„¤ì •(ì£¼ì˜: ì¼ë¶€ Llama ê³„ì—´ì€ eosë¥¼ padë¡œ ì¬ì‚¬ìš©)
    if tok.pad_token_id is None and tok.eos_token_id is not None:
        tok.pad_token_id = tok.eos_token_id
    model = AutoModelForCausalLM.from_pretrained(
        str(mp),
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
        local_files_only=True
    )
    return tok, model

# ---------- ë²• ì´ë¦„ ë§¤ì¹­/ì¿¼ë¦¬ íŒŒì„œ ---------------------------------------------
LAW_ALIASES = {
    "í—Œë²•": ["í—Œë²•", "ëŒ€í•œë¯¼êµ­í—Œë²•"],
    "ë¯¼ë²•": ["ë¯¼ë²•"],
    "í˜•ë²•": ["í˜•ë²•"],
}

def _normalize(s: str) -> str:
    return _nfkc(s).replace(" ", "")

def law_name_matches(row_law: str, want_name: str) -> bool:
    """row_lawê°€ want_name(ë³„ì¹­ í¬í•¨)ê³¼ ì‹¤ì§ˆì ìœ¼ë¡œ ë™ì¼í•œì§€ í™•ì¸"""
    if not row_law or not want_name: return False
    rl = _normalize(row_law)
    for nm in LAW_ALIASES.get(want_name, [want_name]):
        if rl == _normalize(nm):
            return True
    # 'ëŒ€í•œë¯¼êµ­í—Œë²•' vs 'í—Œë²•' ê°™ì€ í¬í•¨ê´€ê³„ í—ˆìš©
    if _normalize(want_name) in rl or rl in _normalize(want_name):
        return True
    return False

LAW_QUERY_RE = re.compile(
    r"(í—Œë²•|ë¯¼ë²•|í˜•ë²•)\s*ì œ?\s*(\d+)\s*ì¡°(?:\s*ì œ?\s*(\d+)\s*í•­)?",
    re.IGNORECASE
)

def parse_law_query(query: str):
    """ì¿¼ë¦¬ì—ì„œ ë²•ë ¹ëª…/ì¡°/í•­(ì˜µì…˜)ì„ ì¶”ì¶œ"""
    m = LAW_QUERY_RE.search(query)
    if not m:
        # 'í—Œë²• 10ì¡°', 'ë¯¼ë²• 245ì¡° 1í•­' í˜•íƒœë„ í—ˆìš©
        m = re.search(r"(í—Œë²•|ë¯¼ë²•|í˜•ë²•)\s*(\d+)\s*ì¡°(?:\s*(\d+)\s*í•­)?", query)
    if not m:
        return None
    want_law = m.group(1)
    want_art = m.group(2)
    want_sub = m.group(3) if len(m.groups()) >= 3 else None
    return {"law": want_law, "article": want_art, "sub": want_sub}

# ---------- â€œì§ì ‘ ì •ë‹µâ€ (LLM ìš°íšŒ) ---------------------------------------------
# íŒíŠ¸/ë²”ìœ„
MULTI_HINT_RE = re.compile(r"(ëª¨ë“ |ì „ë¶€|ì „ì²´|ê°\s*í•­|ìš”ê±´)")
RANGE_RE = re.compile(r"(\d+)\s*[-~]\s*(\d+)\s*í•­")

def _unit_sort_key(r):
    """í•­ ì •ë ¬ í‚¤: í•­ì´ë©´ ìˆ«ì ê¸°ì¤€, ì•„ë‹ˆë©´ ì¡°ê¸ˆ ì•ì— ë‘ê¸°"""
    if r.get("unit") == "í•­":
        raw = r.get("id","").split("::",1)[1] if "::" in r.get("id","") else ""
        sub = _norm_subnum(raw)
        return int(sub) if sub.isdigit() else 10**9
    return 10**9 - 1  # ì¡°ë¬¸ì€ í•­ë“¤ë³´ë‹¤ ì•½ê°„ ì•ìª½

def _format_one_line(r):
    law = r.get("law","ë²•ë ¹")
    art = r.get("article_no","")
    unit = r.get("unit","")
    tail = ""
    if unit == "í•­" and "::" in r.get("id",""):
        sub = _norm_subnum(r["id"].split("::",1)[1])
        tail = f" ì œ{sub}í•­"
    ref = f"{law} ì œ{art}ì¡°{tail}"
    raw = row_text(r)
    text = clean_leading_counter_law(r, raw)
    return f"[{ref}] {text}"

def _pick_first_hang(cands):
    best = None
    bestn = 10**9
    for r in cands:
        if r.get("unit") == "í•­":
            raw = r.get("id","").split("::",1)[1] if "::" in r.get("id","") else ""
            sub = _norm_subnum(raw)
            if sub.isdigit():
                n = int(sub)
                if n < bestn:
                    bestn, best = n, r
    return best

def try_direct_answer(query: str, hits, max_multi: int = 12):
    """
    ì •í™• ë§¤ì¹­ ì‹œ LLM ìš°íšŒ. ë‹¨ì¼/ë²”ìœ„/ì „ì²´í•­ ìë™ ì²˜ë¦¬.
    hits: [(row, kind), ...]
    """
    p = parse_law_query(query)
    if not p:
        return None

    want_law, want_art, want_sub = p["law"], p["article"], p["sub"]
    want_sub_norm = _norm_subnum(want_sub) if want_sub else None

    # í‚¤ì›Œë“œë¡œ "ì „ì²´ í•­" ê°•ì œ
    force_all_keywords = ["ì •ë‹¹í–‰ìœ„", "ì •ë‹¹ë°©ìœ„", "ê¸´ê¸‰í”¼ë‚œ", "ë¶ˆë²•í–‰ìœ„", "ìš”ê±´"]
    want_all = bool(MULTI_HINT_RE.search(query)) or any(k in query for k in force_all_keywords)

    rng = RANGE_RE.search(query)
    want_range = None
    if rng:
        a, b = int(rng.group(1)), int(rng.group(2))
        if a > b: a, b = b, a
        want_range = (a, b)

    # ê°™ì€ ë²•/ê°™ì€ ì¡°ì—ì„œ í›„ë³´ ìˆ˜ì§‘
    cands = []
    for r, kind in hits:
        if kind != "law":
            continue
        if not law_name_matches(r.get("law",""), want_law):
            continue
        if r.get("article_no","") != want_art:
            continue
        unit = r.get("unit","")
        if unit in {"ì¡°ë¬¸", ""} or unit == "í•­":
            cands.append(r)

    if not cands:
        return None

    # ë‹¨ì¼ í•­ ì§€ì •ë§Œ ìˆëŠ” ê²½ìš°
    if want_sub_norm and not want_all and not want_range:
        for r in cands:
            if r.get("unit") == "í•­":
                raw = r.get("id","").split("::",1)[1] if "::" in r.get("id","") else ""
                sub = _norm_subnum(raw)
                if sub == want_sub_norm:
                    return _format_one_line(r)
        # ì •í™• í•­ì´ ì—†ìœ¼ë©´ ì¡°ë¬¸ ë³¸ë¬¸ìœ¼ë¡œ í´ë°±
        for r in cands:
            if r.get("unit") in {"ì¡°ë¬¸", ""}:
                return _format_one_line(r)
        return _format_one_line(cands[0])

    # ë²”ìœ„ ìš”ì²­(ì˜ˆ: 1~3í•­)
    if want_range:
        lo, hi = want_range
        lines = []
        for r in sorted(cands, key=lambda x: _unit_sort_key(x)):
            if r.get("unit") != "í•­":
                continue
            raw = r.get("id","").split("::",1)[1] if "::" in r.get("id","") else ""
            sub = _norm_subnum(raw)
            if sub.isdigit():
                n = int(sub)
                if lo <= n <= hi:
                    lines.append(_format_one_line(r))
            if len(lines) >= max_multi:
                break
        return "\n".join(lines) if lines else None

    # ì „ì²´/ìš”ê±´/ê° í•­ â†’ ê°™ì€ ì¡°ì˜ ëª¨ë“  í•­(ê°€ëŠ¥í•˜ë©´ ì¡°ë¬¸ ë³¸ë¬¸ì€ ë§¨ ìœ„ 1ì¤„ë§Œ)
    if want_all:
        main = None
        for r in cands:
            if r.get("unit") in {"ì¡°ë¬¸", ""}:
                main = r; break
        lines = []
        if main:
            lines.append(_format_one_line(main))
        for r in sorted(cands, key=lambda x: _unit_sort_key(x)):
            if r.get("unit") == "í•­":
                lines.append(_format_one_line(r))
            if len(lines) >= max_multi:
                break
        return "\n".join(lines) if lines else (_format_one_line(main) if main else None)

    # ê¸°ë³¸: ì¡°ë¬¸ 1ì¤„ (ì—†ìœ¼ë©´ 1í•­)
    for r in cands:
        if r.get("unit") in {"ì¡°ë¬¸", ""}:
            return _format_one_line(r)
    first_hang = _pick_first_hang(cands)
    if first_hang:
        return _format_one_line(first_hang)
    return _format_one_line(cands[0])

# ---------- í”„ë¡¬í”„íŠ¸ ë¹Œë” (ë²•ë ¹/íŒë¡€) ------------------------------------------
def build_prompt_law(question: str, contexts):
    ctx_lines=[]
    for i, c in enumerate(contexts, 1):
        law = c.get("law","ë²•ë ¹")
        ref = format_ref_law(c)
        txt = clean_leading_counter_law(c, row_text(c))
        txt = txt.replace("\n"," ").strip()
        ctx_lines.append(f"{i}. [{law} {ref}] {txt}")
    ctx_text = "\n".join(ctx_lines)

    system = (
        "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë ¹ RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "- ë°˜ë“œì‹œ ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ì„¸ìš”(ì™¸ë¶€ì§€ì‹ ê¸ˆì§€).\n"
        "- ì¸ìš©ì€ ë³¸ë¬¸ ì•ˆì— [ë²•ë ¹ëª… ì œXì¡°(ì œYí•­)] í˜•ì‹ìœ¼ë¡œ ìµœì†Œ 1ê°œ ì´ìƒ í‘œê¸°.\n"
        "- ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "- ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ì œê³µëœ ë°œì·Œë¬¸ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
        "- ê³¼ë„í•œ ì¶”ë¡  ê¸ˆì§€. ìµœëŒ€ 5ë¬¸ì¥, ë¶ˆë¦¿ í—ˆìš©."
    )
    user = (
        "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë§Œ í™œìš©í•´ ë‹µí•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸:\n{question}\n\n"
        f"ì»¨í…ìŠ¤íŠ¸:\n{ctx_text}\n\n"
        "ì‘ì„± ì§€ì¹¨:\n"
        "- ì¶œë ¥í˜•ì‹: plain\n"
        "- ë¨¼ì € í•µì‹¬ ê²°ë¡  1~2ë¬¸ì¥, í•„ìš” ì‹œ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ê±´/ê·¼ê±° ì •ë¦¬.\n"
        "- ê° ê·¼ê±° ì˜†ì— [ë²•ë ¹ëª… ì œXì¡°(ì œYí•­)] í˜•íƒœë¡œ ì¸ìš©.\n"
        "\në‹µë³€:"
    )
    return system, user

def build_prompt_case(question: str, contexts):
    ctx_lines=[]
    for i, c in enumerate(contexts, 1):
        ref = format_ref_case(c)  # ex) ëŒ€ë²•ì› 2000-12-22, 2000ë‹¤56259, ê¸‰ì—¬ë“±
        sec = c.get("section","")
        txt = row_text(c).replace("\n"," ").strip()
        ctx_lines.append(f"{i}. [{ref}] ({sec}) {txt}")
    ctx_text = "\n".join(ctx_lines)

    system = (
    "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë ¹Â·íŒë¡€ RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
    "- ë°˜ë“œì‹œ ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'(ë²•ë ¹ ì¡°ë¬¸ ë˜ëŠ” íŒê²°ë¬¸ ë°œì·Œ)ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ì„¸ìš”(ì™¸ë¶€ì§€ì‹ ê¸ˆì§€).\n"
    "- ì§ˆë¬¸ì´ ë²•ë ¹ í•´ì„ì„ ìš”êµ¬í•˜ë©´, ì¡°ë¬¸ì„ ê·¸ëŒ€ë¡œ ê·¼ê±°ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "- ì§ˆë¬¸ì´ íŒë¡€ ê´€ë ¨ì´ë©´, íŒë¡€ì˜ íŒì‹œì‚¬í•­Â·íŒê²°ìš”ì§€Â·íŒë¡€ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n"
    "- ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
    "- ì¸ìš©ì€ ë³¸ë¬¸ ì•ˆì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‘œê¸°:\n"
    "  â€¢ ë²•ë ¹: [ë¯¼ë²• ì œ750ì¡° ì œ1í•­]\n"
    "  â€¢ íŒë¡€: [ëŒ€ë²•ì› 2000-12-22 2000ë‹¤56259, íŒê²°ìš”ì§€]\n"
    "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì‚¬ì‹¤ì´ë‚˜ íŒë‹¨ì€ ì„ì˜ë¡œ ë§Œë“¤ì§€ ë§ê³ , ë¶€ì¡±í•˜ë©´ 'ì œê³µëœ ë°œì·Œë¬¸ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
    "- ìµœëŒ€ 5ë¬¸ì¥ ì´ë‚´ë¡œ, í•„ìš” ì‹œ ë¶ˆë¦¿ìœ¼ë¡œ ì •ë¦¬. í•µì‹¬ ë…¼ì§€Â·ë²•ë¦¬ë§Œ ê°„ê²°íˆ ìš”ì•½."
    )
    user = (
        "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸(íŒë¡€ ë°œì·Œ)ë§Œ í™œìš©í•´ ë‹µí•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸:\n{question}\n\n"
        f"ì»¨í…ìŠ¤íŠ¸:\n{ctx_text}\n\n"
        "ì‘ì„± ì§€ì¹¨:\n"
        "- ì¶œë ¥í˜•ì‹: plain\n"
        "- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ì˜ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "- ë¨¼ì € í•µì‹¬ ê²°ë¡  1~2ë¬¸ì¥, í•„ìš” ì‹œ ë¶ˆë¦¿ë¡œ ë²•ë¦¬/ìš”ê±´/ì‚¬ì‹¤ê´€ê³„ í¬ì¸íŠ¸ ì •ë¦¬.\n"
        "- ì¸ìš©ì€ [ë²•ì›ëª… ì„ ê³ ì¼ì ì‚¬ê±´ë²ˆí˜¸, ì„¹ì…˜] í˜•ì‹.\n"
        "\në‹µë³€:"
    )
    return system, user

def build_prompt_auto(kind: str, question: str, contexts):
    return build_prompt_case(question, contexts) if kind=="case" else build_prompt_law(question, contexts)

# ---------- LLM í…œí”Œë¦¿ ì ìš©/í›„ì²˜ë¦¬ ---------------------------------------------
def apply_chat_or_plain(tok: AutoTokenizer, sys_msg: str, user_msg: str) -> str:
    msgs = [{"role":"system","content":sys_msg},{"role":"user","content":user_msg}]
    if hasattr(tok, "apply_chat_template"):
        try:
            return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        except Exception:
            pass
    return sys_msg + "\n\n" + user_msg + "\n\n"

_STRIP_LINES_RE = re.compile(r"^\s*(\[?(SYSTEM|USER|ASSISTANT)\]?)\s*:?.*$", re.IGNORECASE)
def postprocess_answer(text: str) -> str:
    lines = text.splitlines()
    cleaned=[]
    for ln in lines:
        s=ln.strip()
        if _STRIP_LINES_RE.match(s): continue
        if re.fullmatch(r"\d{2,4}[-/.: ]\d{1,2}[-/.: ]\d{1,2}(?:\s+\d{1,2}:\d{2}(?::\d{2})?)?", s):
            continue
        if s: cleaned.append(ln)
    text="\n".join(cleaned).strip()
    # 3íšŒ ì´ìƒ ë°˜ë³µ ì¤„ ì œê±°
    dedup=[]; prev=None; repeat=0
    for ln in text.splitlines():
        cur=ln.strip()
        if cur==prev:
            repeat+=1
            if repeat>=2: continue
        else:
            prev=cur; repeat=0
        dedup.append(ln)
    return "\n".join(dedup).strip()

# ---------- ë©”ì¸ ----------------------------------------------------------------
def main():
    ROOT = Path(__file__).resolve().parents[1]
    # ë””ë°”ì´ìŠ¤ ê¸°ë³¸ê°’: cuda > mps > cpu
    default_dev = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")

    ap = argparse.ArgumentParser()
    ap.add_argument("-q","--query", required=True)
    ap.add_argument("--index_root", default=str(ROOT / "index_mac"))  # ì´ í´ë” ì•„ë˜ ê° ì½”í¼ìŠ¤ í´ë”
    ap.add_argument("--embed_model", default="BAAI/bge-m3")
    ap.add_argument("--device", default=default_dev)  # ì„ë² ë”©ìš© ë””ë°”ì´ìŠ¤
    ap.add_argument("--topk", type=int, default=6)
    ap.add_argument("--topk_each", type=int, default=6)
    ap.add_argument("--min_score", type=float, default=0.5)
    ap.add_argument("--dedup", action="store_true", help="ê²°ê³¼ ë””ë“‘(ë²•ë ¹: (law,ì¡°,ë‹¨ìœ„) / íŒë¡€: (íŒë¡€ID,ì„¹ì…˜))")
    ap.add_argument("--show_retrieval", action="store_true")
    ap.add_argument("--snippet_chars", type=int, default=160)
    ap.add_argument("--max_ctx", type=int, default=12000)
    ap.add_argument("--llm", default=str(ROOT / "models" / "Meta-Llama-3-8B"))
    ap.add_argument("--max_new_tokens", type=int, default=512)
    args = ap.parse_args()

    index_root = Path(args.index_root)

    # 1) ì¿¼ë¦¬ ì„ë² ë”©
    _, qvec = embed_query(args.query, args.embed_model, device=args.device)

    # 2) ì¸ë±ìŠ¤ ë¡œë“œ & ê²€ìƒ‰
    bundles = load_bundles(index_root)
    merged_hits = retrieve_multi(bundles, qvec, topk_each=args.topk_each)

    # ======================================================================
    # íŒ¨ì¹˜ A â€” ì •í™• ì¡°ë¬¸ ìš°ì„  ë¦¬ë­í¬ (ë²•Â·ì¡°Â·í•­ ì¸ì‹ ì‹œ ê°™ì€ ì¡° ìš°ì„ )
    # ======================================================================
    m = LAW_QUERY_RE.search(args.query) or re.search(r"(í—Œë²•|ë¯¼ë²•|í˜•ë²•)\s*(\d+)\s*ì¡°(?:\s*(\d+)\s*í•­)?", args.query)
    if m:
        want_law, want_art, want_hang = m.group(1), m.group(2), m.group(3)

        def exact_row(h):
            r = h["row"]
            if h["kind"] != "law": return False
            if not law_name_matches(r.get("law"), want_law): return False
            if r.get("article_no") != want_art: return False
            if want_hang:
                return (r.get("unit") == "í•­") and (f"::{_norm_subnum(want_hang)}" in (r.get("id","")) or r.get("id","").endswith(f"::{want_hang}"))
            return True

        def same_article(h):
            r = h["row"]
            return h["kind"]=="law" and law_name_matches(r.get("law"), want_law) and (r.get("article_no")==want_art)

        def same_law(h):
            r = h["row"]
            return h["kind"]=="law" and law_name_matches(r.get("law"), want_law)

        exact = [h for h in merged_hits if exact_row(h)]
        loose = [h for h in merged_hits if h not in exact and same_article(h)]
        same  = [h for h in merged_hits if (h not in exact) and (h not in loose) and same_law(h)]
        other = [h for h in merged_hits if (h not in exact) and (h not in loose) and (h not in same)]
        merged_hits = exact + loose + same + other
    # ======================================================================

    # 3) í•„í„°ë§/ë””ë“‘ & ìµœì¢… ìƒìœ„
    hits=[]; seen=set()
    for h in merged_hits:
        r = h["row"]; s = h["score"]; kind = h["kind"]
        if s < args.min_score: continue
        if args.dedup:
            if kind=="law":
                key=(kind, r.get("law",""), r.get("article_no",""), r.get("unit",""))
            else:  # case
                pid = r.get("íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸") or r.get("_id") or r.get("ì‚¬ê±´ë²ˆí˜¸")
                key=(kind, pid, r.get("section",""))
            if key in seen: continue
            seen.add(key)
        r["_score"]=s
        hits.append((r, kind))
        if len(hits) >= args.topk: break

    # 3.5) ê²€ìƒ‰ê²°ê³¼ í‘œì‹œ
    if args.show_retrieval:
        print("\n=== RETRIEVAL ===")
        if not hits:
            print("(no hits above threshold)")
        else:
            for r, kind in hits:
                if kind=="law":
                    print(format_hit_law(r, r.get("_score",0.0), snippet_chars=args.snippet_chars))
                else:
                    print(format_hit_case(r, r.get("_score",0.0), snippet_chars=args.snippet_chars))

    # ---------- LLM ìš°íšŒ: ì •í™• ì¡°ë¬¸ì´ë©´ ì—¬ê¸°ì„œ ë°”ë¡œ ë‹µ ì¶œë ¥ ----------
    direct = try_direct_answer(args.query, hits)
    if direct:
        print(f"\n(info) selected_ctx={len(hits)} total_chars={sum(len(row_text(r)) for r,_ in hits)} kind=law")
        print("\n=== ANSWER ===")
        print(direct)
        return

    # 4) ì»¨í…ìŠ¤íŠ¸ ëˆ„ì 
    contexts=[]; total_len=0; final_kind="law"
    if hits:
        final_kind = "case" if is_case_row(hits[0][0]) else "law"
    for r, kind in hits:
        t = row_text(r)
        if total_len + len(t) > args.max_ctx: break
        contexts.append(r); total_len += len(t)

    print(f"\n(info) selected_ctx={len(contexts)} total_chars={total_len} kind={final_kind}")

    if not contexts:
        print("\n=== ANSWER ===")
        print("ì œê³µëœ ë°œì·Œë¬¸ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # 5) í”„ë¡¬í”„íŠ¸ êµ¬ì„±(ì½”í¼ìŠ¤ ì¢…ë¥˜ì— ë”°ë¼)
    sys_msg, user_msg = build_prompt_auto(final_kind, args.query, contexts)
    messages = [{"role":"system","content":sys_msg},{"role":"user","content":user_msg}]

    tok, model = load_llm_local_first(args.llm)

    # chat_template ìˆìœ¼ë©´ ì‚¬ìš© (MPSì—ì„œ attention_mask í•„ìˆ˜ â†’ ì§ì ‘ ìƒì„±)
    try:
        inputs_ids_only = tok.apply_chat_template(
            messages, return_tensors="pt", add_generation_prompt=True
        )
        if isinstance(inputs_ids_only, torch.Tensor):
            input_ids = inputs_ids_only.to(model.device)
            attention_mask = torch.ones_like(input_ids)  # MPS ì˜¤ë¥˜ ë°©ì§€: ë§ˆìŠ¤í¬ ìˆ˜ë™ ìƒì„±
        else:
            rendered = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            enc = tok(rendered, return_tensors="pt", padding=True).to(model.device)
            input_ids = enc["input_ids"]
            attention_mask = enc["attention_mask"]
    except Exception:
        # Llama-3 ìˆ˜ë™ í¬ë§·
        def render_llama3(system_text, user_text):
            return ("<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
                    f"{system_text}\n"
                    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n"
                    f"{user_text}\n"
                    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n")
        prompt = render_llama3(sys_msg, user_msg)
        enc = tok(prompt, return_tensors="pt", add_special_tokens=False, padding=True).to(model.device)
        input_ids = enc["input_ids"]
        attention_mask = enc.get("attention_mask", torch.ones_like(input_ids))

    eot_id = tok.convert_tokens_to_ids("<|eot_id|>")
    eos_id = eot_id if (eot_id is not None and eot_id != tok.unk_token_id) else tok.eos_token_id

    out = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,  # ë°˜ë“œì‹œ ì „ë‹¬
        max_new_tokens=args.max_new_tokens,
        do_sample=False,
        eos_token_id=eos_id,
        repetition_penalty=1.05,
    )
    gen_ids = out[0][input_ids.shape[1]:]
    answer = tok.decode(gen_ids, skip_special_tokens=True).strip()
    answer = postprocess_answer(answer)

    print("\n=== ANSWER ===")
    print(answer if answer else "ì œê³µëœ ë°œì·Œë¬¸ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

if __name__=="__main__":
    main()