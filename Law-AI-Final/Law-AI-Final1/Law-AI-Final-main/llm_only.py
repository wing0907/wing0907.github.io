# llm_only.py
import argparse
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def load_llm(model_path: str):
    mp = Path(model_path)
    if not mp.exists():
        raise FileNotFoundError(f"ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {mp}")
    print(f"ğŸ”— ë¡œì»¬ Llama-3 ëª¨ë¸ ë¡œë“œ: {mp}")
    tok = AutoTokenizer.from_pretrained(str(mp), local_files_only=True)
    if tok.pad_token_id is None and tok.eos_token_id is not None:
        tok.pad_token_id = tok.eos_token_id
    model = AutoModelForCausalLM.from_pretrained(
        str(mp),
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
        local_files_only=True
    )
    return tok, model

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("-q", "--query", required=True, help="ì§ˆë¬¸ ë¬¸ì¥")
    ap.add_argument("--llm", default="models/Meta-Llama-3-8B", help="ëª¨ë¸ ê²½ë¡œ")
    ap.add_argument("--max_new_tokens", type=int, default=512)
    args = ap.parse_args()

    tok, model = load_llm(args.llm)

    # ê°„ë‹¨í•œ system/user í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_msg = "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”."
    user_msg = args.query

    # Llama-3 ì „ìš© í”„ë¡¬í”„íŠ¸ í˜•ì‹ (chat template ì—†ì„ ë•Œ ëŒ€ë¹„)
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_msg}\n"
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n"
        f"{user_msg}\n"
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
    )

    inputs = tok(prompt, return_tensors="pt").to(model.device)

    out = model.generate(
        **inputs,
        max_new_tokens=args.max_new_tokens,
        do_sample=True,      # â† ìƒ˜í”Œë§ ì¼œì„œ ë‹¤ì–‘ì„± í™•ë³´
        temperature=0.7,
        top_p=0.9,
        eos_token_id=tok.eos_token_id,
    )

    gen_ids = out[0][inputs["input_ids"].shape[1]:]
    answer = tok.decode(gen_ids, skip_special_tokens=True)
    print("\n=== ANSWER ===")
    print(answer.strip())

if __name__ == "__main__":
    main()